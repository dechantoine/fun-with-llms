{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:29.228657Z",
     "start_time": "2024-02-04T18:44:28.531877Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2Model(\n  (wte): Embedding(50257, 768)\n  (wpe): Embedding(1024, 768)\n  (drop): Dropout(p=0.1, inplace=False)\n  (h): ModuleList(\n    (0-11): 12 x GPT2Block(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPT2Attention(\n        (c_attn): Conv1D()\n        (c_proj): Conv1D()\n        (attn_dropout): Dropout(p=0.1, inplace=False)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPT2MLP(\n        (c_fc): Conv1D()\n        (c_proj): Conv1D()\n        (act): NewGELUActivation()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:30.405708Z",
     "start_time": "2024-02-04T18:44:29.229155Z"
    }
   },
   "id": "7c9464003bc4087f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"This is a very, very, very big prompt. I will always use this same very big prompt to provide context to my autoregressive model language, so let's find out if I can cache it to save computation time or API costs. If only I can precompute the first tokens while waiting the user to answer, it will be so helpful ! Anyway, here is the user input, in case you were wondering:\"\"\"\n",
    "USER_INPUT = \"\"\"Such a nice chatbot. I am just disappointed that it takes so long to respond.\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:30.406116Z",
     "start_time": "2024-02-04T18:44:30.403720Z"
    }
   },
   "id": "c5b678572edad07e",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check consistency between the two forward passes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef96f2e730d841c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer(PROMPT, return_tensors='pt')\n",
    "encoded_user_input = tokenizer(USER_INPUT, return_tensors='pt')\n",
    "encoded_full_input = tokenizer(PROMPT + USER_INPUT, return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:30.412434Z",
     "start_time": "2024-02-04T18:44:30.407204Z"
    }
   },
   "id": "3c5724b2023d6949",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assert torch.equal(torch.cat((encoded_prompt['input_ids'], encoded_user_input['input_ids']), dim=1),\n",
    "                   encoded_full_input['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:30.412686Z",
     "start_time": "2024-02-04T18:44:30.410498Z"
    }
   },
   "id": "b7dea1e56fa71cf1",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "attention_mask = torch.cat((encoded_prompt['attention_mask'], encoded_user_input['attention_mask']), dim=1)\n",
    "prompt_past_kvs = model.forward(**encoded_prompt).past_key_values\n",
    "\n",
    "concat_output = model.forward(input_ids=encoded_user_input['input_ids'],\n",
    "                              attention_mask=attention_mask,\n",
    "                              past_key_values=prompt_past_kvs)\n",
    "full_output = model.forward(**encoded_full_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:30.771525Z",
     "start_time": "2024-02-04T18:44:30.413009Z"
    }
   },
   "id": "8fe318de2d81b9da",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "assert torch.allclose(concat_output.last_hidden_state,\n",
    "                      full_output.last_hidden_state[0, -concat_output.last_hidden_state.shape[1]:, :],\n",
    "                      atol=1e-04)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T18:44:30.775583Z",
     "start_time": "2024-02-04T18:44:30.771745Z"
    }
   },
   "id": "9696ee1862c174ec",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time measure"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7794f80257677502"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 31.3 s, total: 1min 43s\n",
      "Wall time: 47.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(1000):\n",
    "    encoded_full_input = tokenizer(PROMPT + USER_INPUT, return_tensors='pt')\n",
    "    model.forward(**encoded_full_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T20:19:09.782535Z",
     "start_time": "2024-02-04T20:18:21.891510Z"
    }
   },
   "id": "d6f3a445ce649227",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 7.89 s, total: 30.9 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "encoded_prompt = tokenizer(PROMPT, return_tensors='pt')\n",
    "prompt_past_kvs = model.forward(**encoded_prompt)['past_key_values']\n",
    "\n",
    "for i in range(1000):\n",
    "    encoded_user_input = tokenizer(USER_INPUT, return_tensors='pt')\n",
    "    attention_mask = torch.cat((encoded_prompt['attention_mask'], encoded_user_input['attention_mask']), dim=1)\n",
    "    model.forward(input_ids=encoded_user_input['input_ids'],\n",
    "                  attention_mask=attention_mask,\n",
    "                  past_key_values=prompt_past_kvs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T20:19:30.389856Z",
     "start_time": "2024-02-04T20:19:09.783623Z"
    }
   },
   "id": "80d3c7e2f884640d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9e674a1936bf5eee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
